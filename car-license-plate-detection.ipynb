{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### LICENSE PLACE DETECTION AND RECOGNITION","metadata":{}},{"cell_type":"code","source":"# !pip install albumentations==0.4.6\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport ast\n\nimport numpy as np \nimport pandas as pd \n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.image as immg\n\nfrom sklearn import model_selection\n\nimport random\n\nimport torch\n\nimport torchvision\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.transforms as T\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection import FasterRCNN\nfrom tqdm.notebook import tqdm\n\nimport warnings\nimport os\nimport random\nwarnings.filterwarnings(\"ignore\")\nfrom glob import glob\nimport xml.etree.ElementTree as xet\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed=42):\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nset_seed()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = glob(\"../input/car-plate-detection/annotations/*.xml\")\nlabels_dict = dict(filepath=[],xmin=[],xmax=[],ymin=[],ymax=[])\n\nfor filename in path:\n    data = xet.parse(filename)\n    root = data.getroot()\n    obj = root.find('object')\n    labels_info = obj.find('bndbox')\n    xmin = int(labels_info.find('xmin').text)\n    xmax = int(labels_info.find('xmax').text)\n    ymin = int(labels_info.find('ymin').text)\n    ymax = int(labels_info.find('ymax').text)\n    labels_dict['filepath'].append(filename)\n    labels_dict['xmin'].append(xmin)\n    labels_dict['xmax'].append(xmax)\n    labels_dict['ymin'].append(ymin)\n    labels_dict['ymax'].append(ymax)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_df = pd.DataFrame(labels_dict)\nvalid_df.to_csv('labels.csv',index=False)\nvalid_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_df['filepath'] = valid_df['filepath'].str.replace('../input/car-plate-detection/annotations/','')\nvalid_df['filepath'] = valid_df['filepath'].str.replace('.xml','.png')\nvalid_df['class'] = 'license'\nvalid_df = valid_df.rename(columns = {'filepath':'img_id'})\nvalid_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/license-plates/license_plates_detection_train.csv')\ndf['class'] = 'license'\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape, valid_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classes_la = {\"license\": 1}\n\ndf[\"class\"] = df[\"class\"].apply(lambda x: classes_la[x])\nvalid_df[\"class\"] = valid_df[\"class\"].apply(lambda x: classes_la[x])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plotting the Images","metadata":{}},{"cell_type":"code","source":"# group by all bounding boxes (bbox)\ndf_grp = df.groupby(['img_id'])\ndf_grp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = '/kaggle/input/license-plates/license_plates/license_plates/license_plates_detection_train/license_plates_detection_train/'\ndef plot_image(image_name):\n    image_group = df_grp.get_group(image_name)\n    bbox = image_group.loc[:,['xmin', 'ymin', 'xmax', 'ymax']]\n    img = immg.imread(path+name)\n    fig,ax = plt.subplots(figsize=(18,10))\n    ax.imshow(img,cmap='binary')\n    for i in range(len(bbox)):\n        box = bbox.iloc[i].values\n        print(bbox)\n        x,y,w,h = box[0], box[1], box[2]-box[0], box[3]-box[1]\n        rect = matplotlib.patches.Rectangle((x,y),w,h,linewidth=1,edgecolor='r',facecolor='none',)\n        ax.text(*box[:2], image_group[\"class\"].values, verticalalignment='top', color='white', fontsize=13, weight='bold')\n        ax.add_patch(rect)\n    plt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"name = df.img_id.unique()[0]\nplot_image(name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"name = df.img_id.unique()[700]\nplot_image(name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"name = df.img_id.unique()[67]\nplot_image(name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create a Custom Dataset","metadata":{}},{"cell_type":"code","source":"class LicensePlate(object):\n    def __init__(self, df, IMG_DIR, transforms):\n        self.df = df\n        self.img_dir = IMG_DIR\n        self.image_ids = self.df.img_id.unique()\n        self.transforms = transforms\n        \n    def __len__(self):\n        return len(self.image_ids)\n    \n    def __getitem__(self, idx):\n        image_id = self.image_ids[idx]\n        image_values = self.df[self.df['img_id'] == image_id]\n        image = cv2.imread(self.img_dir+image_id,cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image /=255.0\n        boxes = image_values[['xmin', 'ymin', 'xmax', 'ymax']].to_numpy()\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        labels = image_values[\"class\"].values\n        labels = torch.tensor(labels)\n        target = {}\n        target['boxes'] = boxes\n        target['labels'] = labels\n        target['image_id'] = torch.tensor([idx])\n        target['area'] = torch.as_tensor(area, dtype=torch.float32)\n        target['iscrowd'] = torch.zeros(len(classes_la), dtype=torch.int64)        \n        \n        if self.transforms:\n            sample = {\n                'image':image,\n                'bboxes': target['boxes'],\n                'labels': labels\n            }\n            \n        sample = self.transforms(**sample)\n        image = sample['image']\n        target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n        \n        return torch.tensor(image), target, image_id\n        \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### AUGMENTATIONS","metadata":{}},{"cell_type":"code","source":"def get_train_transform():\n    return A.Compose([\n        A.Resize(512,512),\n        A.Transpose(),\n        A.RandomBrightnessContrast(\n            brightness_limit=(-0.1,0.1), \n            contrast_limit=(-0.1, 0.1), \n            p=0.5\n        ),\n        A.HueSaturationValue(\n            hue_shift_limit=0.2, \n            sat_shift_limit=0.2, \n            val_shift_limit=0.2, \n            p=0.5\n            ), \n        A.Blur(p=0.5),\n\n\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n\ndef get_valid_transform():\n    return A.Compose([\n        A.Resize(512, 512), \n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LOAD LICENSE PLATE DATASET AND PERFORM THE TRANSFORMATIONS","metadata":{}},{"cell_type":"code","source":"license_dataset = LicensePlate(df, path, get_train_transform())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img, tar, _ = license_dataset[random.randint(0,906)]\nbbox = tar['boxes']\nfig,ax = plt.subplots(figsize=(18,10))\nax.imshow(img.permute(1,2,0).cpu().numpy())\nfor l in tar[\"labels\"].tolist():\n    classes_la = {1:\"license\"}\n    l = classes_la[l]\n    for i in range(len(bbox)):\n        box = bbox[i]\n        x,y,w,h = box[0], box[1], box[2]-box[0], box[3]-box[1]\n        rect = matplotlib.patches.Rectangle((x,y),w,h,linewidth=2,edgecolor='r',facecolor='none',)\n#         ax.text(*box[:2], l, verticalalignment='top', color='red', fontsize=13, weight='bold') # Useful if you have many classes\n        ax.add_patch(rect)\n    plt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Split data into Train and Test","metadata":{}},{"cell_type":"code","source":"display(df.head(),\n        df.shape,\n        valid_df.head(),\n        valid_df.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### DATALOADER","metadata":{}},{"cell_type":"code","source":"def collate_fn(batch):\n    return tuple(zip(*batch))\nvalid_path = '/kaggle/input/car-plate-detection/images/'\n\ntrain_dataset = LicensePlate(df, path, get_train_transform())\nvalid_dataset = LicensePlate(valid_df, valid_path, get_valid_transform())\n\n#split the dataset into train and test set\nindices = torch.randperm(len(train_dataset)).tolist()\n\ntrain_data_loader = DataLoader(\n    train_dataset,\n    batch_size = 8,\n    shuffle = False,\n    num_workers = 4,\n    collate_fn = collate_fn\n)\n\nvalid_data_loader = DataLoader(\n    valid_dataset,\n    batch_size = 8,\n    shuffle = False,\n    num_workers = 4,\n    collate_fn = collate_fn\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### model","metadata":{}},{"cell_type":"code","source":"num_classes = 2 #because we have to add the background class to the license plate\n\n# load a model; pre-trained on COCO\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n\n# get number of input features for the classifier\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n\n# replace the pre-trained head with a new one\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\ndevice = torch.device('cuda') if torch.cuda.is_available else torch.device('cpu')\nnum_epochs = 10","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.to(device)\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n# lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, len(train_data_loader)*num_epochs)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training and Evaluation","metadata":{}},{"cell_type":"code","source":"import sys\nbest_epoch = 0\nmin_loss = sys.maxsize\nbest_IOU =0.5\nes_patience = 3\n\nfor epoch in range(num_epochs):\n    tk = tqdm(train_data_loader)\n    model.train(); #training mode\n    for images, targets, image_ids in tk:\n        images = list(image.to(device) for image in images )\n        targets = [{k: v.long().to(device) for k, v in t.items()} for t in targets]\n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n        loss_value = losses.item()\n        \n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n        \n        tk.set_postfix(train_loss = loss_value)\n    tk.close()\n    \n    #update the learning rate\n    if lr_scheduler is not None:\n        lr_scheduler.step()\n        \n    print(f\"Epoch #{epoch} loss: {loss_value}\") \n    \n    #validation\n    model.eval();\n    with torch.no_grad():\n        tk = tqdm(valid_data_loader)\n        for images, targets, image_ids in tk:\n            images = list(image.to(device) for image in images)\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n            val_output = model(images)\n            val_output = [{k: v.to('cpu') for k, v in t.items()} for t in val_output]\n            IOU = []\n            \n            for j in range(len(val_output)):\n                a,b = val_output[j]['boxes'].cpu().detach(), targets[j]['boxes'].cpu().detach()\n                chk = torchvision.ops.box_iou(a,b)\n                res = np.nanmean(chk.sum(axis=1)/(chk>0).sum(axis=1))\n                IOU.append(res)\n            tk.set_postfix(IoU=np.mean(IOU))\n        tk.close()\n        print(res)\n        if res > best_IOU:\n            best_IOU = res\n            patience = es_patience\n            # Resetting patience since we have new best validation accuracy\n            print(\" Saving the best model\")\n            torch.save(model.state_dict(),f'best_fasterrcnn_updated.pth')  # Saving current best model\n                    \n        else:\n            patience -= 1\n            if patience == 0:\n                print('Early stopping. Best Val IOU: {:.3f}'.format(best_IOU))\n                break               \n\n            \n                \n            \n        \n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluation on sample image","metadata":{}},{"cell_type":"code","source":"img,target,_ = valid_dataset[5]\n# put the model in evaluation mode\nmodel.eval()\nwith torch.no_grad():\n    prediction = model([img.to(device)])[0]\n    \nprint('predicted #boxes: ', len(prediction['boxes']))\n\nprint('real #boxes: ', len(target['boxes']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Ground truth box for the sample valid image","metadata":{}},{"cell_type":"code","source":"bbox = target['boxes'].numpy()\nfig,ax = plt.subplots(1,figsize=(18,10))\nax.imshow(img.permute(1,2,0).cpu().numpy())\nfor l in target[\"labels\"]:\n    for i in range(len(bbox)):\n        box = bbox[i]\n        x,y,w,h = box[0], box[1], box[2]-box[0], box[3]-box[1]\n        rect = matplotlib.patches.Rectangle((x,y),w,h,linewidth=2,edgecolor='r',facecolor='none',)\n#         ax.text(*box[:2], l.tolist(), verticalalignment='top', color='red', fontsize=13, weight='bold')\n        ax.add_patch(rect)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Predicted box for the sample valid Image","metadata":{}},{"cell_type":"code","source":"def plot_valid(img,prediction,nms=True,detect_thresh=0.5):\n    fig,ax = plt.subplots(figsize=(18,10))\n    val_img = img.permute(1,2,0).cpu().numpy()\n    ax.imshow(val_img)\n    val_scores = prediction['scores'].cpu().detach().numpy()\n    bbox = prediction['boxes'].cpu().detach().numpy()\n    for l in target[\"labels\"]:\n        for i in range(len(bbox)):\n            if val_scores[i]>=detect_thresh:\n                box = bbox[i]\n                x,y,w,h = box[0], box[1], box[2]-box[0], box[3]-box[1]\n                rect = matplotlib.patches.Rectangle((x,y),w,h,linewidth=2 ,edgecolor='r',facecolor='none',)\n#                 ax.text(*box[:2], \"class \" + str(l.tolist()) + \" score {0:.3f}\".format(val_scores[i]), verticalalignment='top', color='white', fontsize=12, weight='bold')\n                ax.add_patch(rect)\n        plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_valid(img,prediction)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Loading The Test Images","metadata":{}},{"cell_type":"code","source":"submission = pd.read_csv('/kaggle/input/license-plates/license_plates/license_plates/SampleSubmission.csv')\nsubmission['img_id'] = submission['filepath'].str.extract(r'img_(\\d+)_\\d+')\nsub = submission[['img_id']]\nsub = sub.drop_duplicates().reset_index(drop=True)\nsub.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class One_Image(object):\n    def __init__(self,IMG_PATH, transforms):        \n        \n        self.img_path = IMG_PATH\n        self.transforms = transforms\n\n    \n    def __getitem__(self, idx=0):        \n        image = cv2.imread(self.img_path,cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        \n        if self.transforms:\n            sample = {\n                'image': image,\n            }\n            sample = self.transforms(**sample)\n            image = sample['image']\n        return image","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMG_SIZE = (512,512)\ndef get_test_transform(IMG_SIZE=(512,512)):\n    return A.Compose([\n         A.Resize(*IMG_SIZE),\n         A.Normalize(mean=(0, 0, 0), std=(1, 1, 1), max_pixel_value=255.0, p=1.0),\n        \n        ToTensorV2(p=1.0)\n    ])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_path = '/kaggle/input/license-plates/license_plates/license_plates/test/test_private/'\none_dataset = One_Image(test_path + sub.img_id[random.randint(0,200)], get_test_transform())\none_dataset","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plot the predicted test Image","metadata":{}},{"cell_type":"code","source":"img = one_dataset[0]\n# put the model in evaluation mode\nmodel.eval()\nwith torch.no_grad():\n    prediction = model([img.to(device)])[0]\n    \nprint('predicted #boxes: ', len(prediction['boxes']))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_valid(img,prediction,nms=True,detect_thresh=0.5):\n    fig,ax = plt.subplots(figsize=(18,10))\n    val_img = one_dataset[0].permute(1,2,0).cpu().numpy()\n    ax.imshow(val_img)\n    val_scores = prediction['scores'].cpu().detach().numpy()\n    bbox = prediction['boxes'].cpu().detach().numpy()\n    \n    for i in range(len(bbox)):\n        if val_scores[i]>=detect_thresh:\n            box = bbox[i]\n            x,y,w,h = box[0], box[1], box[2]-box[0], box[3]-box[1]\n            rect = matplotlib.patches.Rectangle((x,y),w,h,linewidth=2 ,edgecolor='r',facecolor='none',)\n#             ax.text(*box[:2], \"class \" + str(l.tolist()) + \" score {0:.3f}\".format(val_scores[i]), verticalalignment='top', color='white', fontsize=12, weight='bold')\n            ax.add_patch(rect)\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]}]}